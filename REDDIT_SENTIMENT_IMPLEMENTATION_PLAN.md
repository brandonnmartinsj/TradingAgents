# üìã Plano de Implementa√ß√£o: An√°lise de Sentimento em Tempo Real (Reddit)

## üîç An√°lise do Estado Atual

**Descobertas:**
- ‚úÖ Projeto j√° tem `praw` (Python Reddit API Wrapper) como depend√™ncia
- ‚úÖ Existe `reddit_utils.py` com fun√ß√µes para buscar posts (mas usa dados locais offline)
- ‚úÖ Existe `social_media_analyst.py` mas n√£o usa Reddit diretamente
- ‚úÖ Dashboard j√° tem an√°lise de sentimento b√°sica em `news_utils.py` (keyword-based)
- ‚ö†Ô∏è N√£o h√° integra√ß√£o com Reddit API em tempo real
- ‚ö†Ô∏è N√£o h√° credenciais Reddit no `.env`

---

## üèóÔ∏è Arquitetura Proposta

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    WEB DASHBOARD                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  üìä Sentiment Page (NEW)                           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Real-time sentiment score                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Trending tickers on Reddit                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Sentiment timeline chart                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Top posts with sentiment                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Word cloud                                      ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  üè† Dashboard Page (ENHANCED)                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Add Reddit sentiment badge to ticker cards      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Sentiment trend indicator                       ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚ñ≤
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           utils/reddit_sentiment_utils.py (NEW)             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  RedditClient                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - fetch_posts(ticker, subreddits, limit)           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - fetch_trending_tickers()                         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  SentimentAnalyzer                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - analyze_text(text) -> score                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - batch_analyze(posts) -> results                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - get_sentiment_trend(ticker, hours=24)            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚ñ≤
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  EXTERNAL SERVICES                          ‚îÇ
‚îÇ  - Reddit API (PRAW)                                        ‚îÇ
‚îÇ  - Subreddits: r/wallstreetbets, r/stocks, r/investing     ‚îÇ
‚îÇ  - VADER Sentiment Analysis (nltk)                          ‚îÇ
‚îÇ  - TextBlob (optional fallback)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìù Plano Detalhado de Implementa√ß√£o

### **FASE 1: Setup e Infraestrutura** ‚è±Ô∏è ~30min

#### 1.1 Configura√ß√£o de Credenciais Reddit
**Arquivo**: `.env`, `settings.example.json`

```bash
# Adicionar ao .env
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USER_AGENT=TradingAgents/1.0
```

**Tarefas**:
- [ ] Adicionar campos Reddit em `settings.example.json`
- [ ] Atualizar `.env.example` com vari√°veis Reddit
- [ ] Documentar como obter credenciais Reddit API

#### 1.2 Atualizar Depend√™ncias
**Arquivo**: `web_dashboard/requirements.txt`

```txt
# Adicionar:
praw>=7.7.0           # Reddit API
vaderSentiment>=3.3.2 # Sentiment analysis optimized for social media
textblob>=0.17.1      # Alternative sentiment analysis
wordcloud>=1.9.0      # Word cloud visualization
nltk>=3.8.0           # NLP toolkit
```

**Tarefas**:
- [ ] Adicionar depend√™ncias ao requirements.txt
- [ ] Testar instala√ß√£o: `pip install -r web_dashboard/requirements.txt`

---

### **FASE 2: M√≥dulo Reddit Sentiment** ‚è±Ô∏è ~2-3h

#### 2.1 Criar `utils/reddit_sentiment_utils.py`

**Componentes principais**:

```python
# Estrutura do arquivo:

1. RedditClient Class
   - __init__(client_id, client_secret, user_agent)
   - fetch_posts(ticker, subreddits, limit, time_filter)
   - fetch_trending_tickers(subreddits, limit)
   - get_company_mentions(ticker, hours=24)

2. SentimentAnalyzer Class
   - __init__(method='vader')  # vader, textblob, hybrid
   - analyze_text(text) -> {'score': float, 'label': str, 'confidence': float}
   - batch_analyze(posts) -> List[dict]
   - get_aggregate_sentiment(posts) -> dict
   - get_sentiment_trend(ticker, hours=24) -> timeseries

3. Utility Functions
   - normalize_ticker(text) -> List[str]  # Extract $AAPL, AAPL mentions
   - clean_text(text) -> str
   - calculate_post_weight(upvotes, comments, awards) -> float
   - get_sentiment_emoji(score) -> str
   - get_sentiment_color(score) -> str
```

**Funcionalidades**:
- ‚úÖ Busca posts de m√∫ltiplos subreddits
- ‚úÖ An√°lise de sentimento com VADER (otimizado para social media)
- ‚úÖ Peso por engagement (upvotes + comments + awards)
- ‚úÖ Cache com Streamlit @st.cache_data (TTL 15min)
- ‚úÖ Rate limiting para Reddit API
- ‚úÖ Error handling robusto

---

### **FASE 3: P√°gina de Sentimento** ‚è±Ô∏è ~3-4h

#### 3.1 Criar `pages/reddit_sentiment.py`

**Layout da p√°gina**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üìä Reddit Sentiment Analysis                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ Avg Score   ‚îÇ ‚îÇ Total Posts ‚îÇ ‚îÇ Bullish %   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ    +0.45    ‚îÇ ‚îÇ     156     ‚îÇ ‚îÇ    68%      ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [Ticker Selector: AAPL ‚ñº]  [Timeframe: 24h ‚ñº]         ‚îÇ
‚îÇ  [Subreddits: ‚òë WSB ‚òë Stocks ‚òë Investing]              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìà Sentiment Timeline (Plotly Line Chart)              ‚îÇ
‚îÇ     - Sentiment score over time                         ‚îÇ
‚îÇ     - Volume bars below                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üî• Trending Tickers (Today)                            ‚îÇ
‚îÇ     1. NVDA  üü¢ +0.62  (234 mentions)                   ‚îÇ
‚îÇ     2. TSLA  üî¥ -0.21  (189 mentions)                   ‚îÇ
‚îÇ     3. AAPL  üü¢ +0.45  (156 mentions)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üí¨ Top Reddit Posts                                    ‚îÇ
‚îÇ     [Card with sentiment badge, upvotes, comments]      ‚îÇ
‚îÇ     [Card with sentiment badge, upvotes, comments]      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚òÅÔ∏è Word Cloud (Most mentioned terms)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìä Sentiment Distribution (Pie Chart)                  ‚îÇ
‚îÇ     - Bullish / Neutral / Bearish                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Tabs dentro da p√°gina**:
1. **Overview**: M√©tricas principais + timeline
2. **Posts**: Lista de posts com sentimento
3. **Trends**: Trending tickers + word cloud
4. **Analytics**: Distribui√ß√£o, correla√ß√µes

**Tarefas**:
- [ ] Implementar layout responsivo
- [ ] Criar visualiza√ß√µes Plotly interativas
- [ ] Implementar filtros (ticker, timeframe, subreddits)
- [ ] Adicionar export de dados (CSV, JSON)
- [ ] Implementar auto-refresh opcional

---

### **FASE 4: Integra√ß√£o com Dashboard Existente** ‚è±Ô∏è ~1-2h

#### 4.1 Atualizar `app.py`
```python
# Adicionar p√°gina ao menu
page = st.sidebar.radio("Select Page", [
    "üè† Dashboard",
    "üìÑ Report Viewer",
    "üîç Comparison",
    "üíº Portfolio",
    "üîî Alerts",
    "üìà Analytics",
    "ü§ñ Reddit Sentiment",  # NEW
    "‚öôÔ∏è Settings"
])
```

#### 4.2 Atualizar `pages/dashboard.py`
- Adicionar badge de sentimento Reddit nos ticker cards
- Mostrar trending status (üî• if trending)
- Link para p√°gina de sentimento detalhado

```python
# Exemplo de badge:
sentiment_score = get_reddit_sentiment(ticker)
if sentiment_score:
    st.markdown(f"Reddit: {get_sentiment_emoji(sentiment_score)} {sentiment_score:+.2f}")
```

#### 4.3 Atualizar `pages/settings.py`
- Adicionar se√ß√£o "Reddit API" para configurar credenciais
- Sele√ß√£o de subreddits default
- Configura√ß√£o de cache TTL

---

### **FASE 5: Visualiza√ß√µes e M√©tricas** ‚è±Ô∏è ~2h

#### 5.1 Gr√°ficos Plotly

**1. Sentiment Timeline**
```python
def create_sentiment_timeline(data):
    fig = go.Figure()

    # Line chart: sentiment score over time
    fig.add_trace(go.Scatter(
        x=data['timestamp'],
        y=data['sentiment_score'],
        mode='lines+markers',
        name='Sentiment',
        line=dict(color='rgb(75, 192, 192)')
    ))

    # Bar chart: post volume
    fig.add_trace(go.Bar(
        x=data['timestamp'],
        y=data['post_count'],
        name='Volume',
        yaxis='y2',
        opacity=0.3
    ))

    fig.update_layout(
        title='Reddit Sentiment Timeline',
        yaxis=dict(title='Sentiment Score'),
        yaxis2=dict(title='Post Volume', overlaying='y', side='right')
    )

    return fig
```

**2. Sentiment Distribution (Pie Chart)**
```python
def create_sentiment_distribution(sentiments):
    bullish = sum(1 for s in sentiments if s > 0.15)
    neutral = sum(1 for s in sentiments if -0.15 <= s <= 0.15)
    bearish = sum(1 for s in sentiments if s < -0.15)

    fig = go.Figure(data=[go.Pie(
        labels=['üü¢ Bullish', '‚ö™ Neutral', 'üî¥ Bearish'],
        values=[bullish, neutral, bearish],
        marker=dict(colors=['#10b981', '#6b7280', '#ef4444'])
    )])

    return fig
```

**3. Trending Tickers Heatmap**
```python
def create_trending_heatmap(tickers_data):
    # Heatmap: ticker x time with sentiment intensity
    fig = go.Figure(data=go.Heatmap(
        z=sentiment_matrix,
        x=timestamps,
        y=tickers,
        colorscale='RdYlGn'
    ))

    return fig
```

**4. Word Cloud**
```python
from wordcloud import WordCloud

def create_word_cloud(texts):
    text = ' '.join(texts)
    wordcloud = WordCloud(width=800, height=400).generate(text)

    fig, ax = plt.subplots()
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')

    return fig
```

---

### **FASE 6: Testes e Valida√ß√£o** ‚è±Ô∏è ~1h

#### 6.1 Testes Unit√°rios

**Arquivo**: `web_dashboard/tests/test_reddit_sentiment.py`

```python
def test_reddit_client_fetch():
    # Test fetching posts

def test_sentiment_analyzer():
    # Test VADER sentiment

def test_ticker_extraction():
    # Test $AAPL extraction

def test_weighted_sentiment():
    # Test engagement weighting
```

#### 6.2 Testes de Integra√ß√£o
- [ ] Testar com API keys reais
- [ ] Validar rate limiting
- [ ] Testar cache behavior
- [ ] Validar visualiza√ß√µes

#### 6.3 Valida√ß√£o de Performance
- [ ] Medir tempo de carregamento
- [ ] Otimizar queries Reddit
- [ ] Validar uso de cache

---

## üìä Estrutura de Dados

### Reddit Post Object
```python
{
    'id': 'abc123',
    'title': 'AAPL to the moon! üöÄ',
    'text': 'Apple just announced...',
    'ticker': 'AAPL',
    'subreddit': 'wallstreetbets',
    'created_utc': 1234567890,
    'upvotes': 1250,
    'num_comments': 87,
    'awards': 5,
    'url': 'https://reddit.com/...',
    'sentiment': {
        'score': 0.652,
        'label': 'bullish',
        'confidence': 0.87
    },
    'weight': 0.45  # Based on engagement
}
```

### Sentiment Aggregate Object
```python
{
    'ticker': 'AAPL',
    'period': '24h',
    'avg_sentiment': 0.45,
    'weighted_sentiment': 0.52,
    'total_posts': 156,
    'bullish_count': 106,
    'neutral_count': 32,
    'bearish_count': 18,
    'total_engagement': 45230,  # upvotes + comments
    'trending_rank': 3,
    'sentiment_trend': 'increasing',  # vs previous period
    'top_keywords': ['earnings', 'vision', 'ai'],
    'timestamp': '2025-10-15T14:00:00Z'
}
```

---

## üéØ Subreddits Target

**Prim√°rios** (alta qualidade):
- `r/wallstreetbets` - 15M members, high volume, meme stocks
- `r/stocks` - 6M members, serious discussion
- `r/investing` - 2M members, long-term focus

**Secund√°rios** (opcional):
- `r/StockMarket` - 2M members
- `r/options` - 400K members
- `r/pennystocks` - 300K members

---

## ‚öôÔ∏è Configura√ß√µes Recomendadas

```json
// settings.json
{
  "reddit": {
    "enabled": true,
    "subreddits": ["wallstreetbets", "stocks", "investing"],
    "posts_per_subreddit": 50,
    "time_filter": "day",
    "cache_ttl": 900,
    "sentiment_method": "vader",
    "min_upvotes": 10,
    "show_trending": true
  }
}
```

---

## üìà M√©tricas de Sucesso

- ‚úÖ Lat√™ncia < 3s para carregar sentimento
- ‚úÖ Cache hit rate > 80%
- ‚úÖ Accuracy sentimento > 75% (vs manual labeling)
- ‚úÖ Suporte para 100+ posts simult√¢neos
- ‚úÖ Zero crashes em 24h de opera√ß√£o

---

## üöÄ Roadmap Futuro (P√≥s-MVP)

**V2 Features**:
- Sentiment alerts (notificar quando sentimento muda drasticamente)
- Historical sentiment data storage (SQLite/PostgreSQL)
- Correlation analysis (sentiment vs price movement)
- Multi-language support (PT-BR posts)
- Advanced NLP (FinBERT, custom fine-tuned models)
- Reddit user influence scoring (weight by karma/age)

---

## ‚úÖ Checklist de Implementa√ß√£o

### Fase 1: Setup
- [ ] Adicionar credenciais Reddit ao `.env`
- [ ] Atualizar `requirements.txt`
- [ ] Instalar depend√™ncias

### Fase 2: Core Module
- [ ] Implementar `RedditClient` class
- [ ] Implementar `SentimentAnalyzer` class
- [ ] Adicionar utility functions
- [ ] Implementar caching

### Fase 3: UI
- [ ] Criar `reddit_sentiment.py` page
- [ ] Implementar tabs (Overview, Posts, Trends, Analytics)
- [ ] Adicionar filtros e controles

### Fase 4: Integra√ß√£o
- [ ] Atualizar `app.py` navigation
- [ ] Adicionar badges em `dashboard.py`
- [ ] Adicionar settings em `settings.py`

### Fase 5: Visualiza√ß√µes
- [ ] Sentiment timeline chart
- [ ] Distribution pie chart
- [ ] Trending heatmap
- [ ] Word cloud

### Fase 6: Testes
- [ ] Testes unit√°rios
- [ ] Testes de integra√ß√£o
- [ ] Valida√ß√£o de performance
- [ ] Documenta√ß√£o

---

## üìö Como Obter Credenciais Reddit API

1. Acesse https://www.reddit.com/prefs/apps
2. Clique em "create another app..." ou "create app"
3. Preencha:
   - **name**: TradingAgents
   - **App type**: script
   - **description**: Real-time sentiment analysis for stocks
   - **about url**: (deixe em branco)
   - **redirect uri**: http://localhost:8080
4. Clique em "create app"
5. Copie as credenciais:
   - **client_id**: string abaixo de "personal use script"
   - **client_secret**: string ao lado de "secret"
   - **user_agent**: "TradingAgents/1.0"

---

**Tempo Total Estimado**: 8-12 horas

**Ordem de Prioridade**:
1. Fase 1 + 2 (Core functionality)
2. Fase 3 (Basic UI)
3. Fase 4 (Integration)
4. Fase 5 (Polish visualizations)
5. Fase 6 (Testing)

---

**Data de Cria√ß√£o**: 2025-10-15
**Vers√£o**: 1.0
**Autor**: Claude Code + Brandon
